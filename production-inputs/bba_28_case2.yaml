experiment_directory: /lus/theta-fs0/projects/RL-fold/msalim/production-runs/bba_28_case2
walltime_min: 150

md_runner:
    frames_per_h5: 64
    initial_configs_dir: /lus/theta-fs0/projects/RL-fold/msalim/SC20-GB-CS1-ThetaGPU-AI-driven-MD/data/1FME
    local_run_dir: /raid/scratch
    md_environ_setup:
        - eval "$(/lus/theta-fs0/projects/RL-fold/msalim/miniconda3/bin/conda shell.bash
          hook)"
        - conda activate /lus/theta-fs0/projects/RL-fold/venkatv/software/conda_env/a100_rapids_openmm
        - export PYTHONPATH=/lus/theta-fs0/projects/RL-fold/msalim/SC20-GB-CS1-ThetaGPU-AI-driven-MD:$PYTHONPATH
    md_run_command: /lus/theta-fs0/projects/RL-fold/venkatv/software/conda_env/a100_rapids_openmm/bin/python -m deepdrivemd.sim.run_openmm
    num_jobs: 160
    temperature_kelvin: 310.0
    reference_pdb_file: /lus/theta-fs0/projects/RL-fold/msalim/SC20-GB-CS1-ThetaGPU-AI-driven-MD/data/1FME/1FME-folded.pdb
    report_interval_ps: 50.0
    sim_type: implicit
    simulation_length_ns: 10.0

outlier_detection:
    environ_setup: []
    extrinsic_outlier_score: rmsd_to_reference_state
    gpus_per_node: 8
    local_scratch_dir: /raid/scratch
    max_num_old_h5_files: 40
    num_nodes: 1
    num_outliers: 500
    ranks_per_node: 1
    run_command: "singularity run -B /lus:/lus:rw -B /raid:/raid:rw --nv /lus/theta-fs0/projects/RL-fold/msalim/tensorflow_20.09-tf1-py3.sif /lus/theta-fs0/projects/RL-fold/msalim/tf1-ngc-env/bin/python -m deepdrivemd.outlier.lof"
    sklearn_num_cpus: 16

cs1_training: # start with 10240 and supplement with 10240 at a time (2048 eval, 8192 train)
    eval_steps: 8 # 512*8=4096 samples
    hostname: medulla1
    medulla_experiment_path: /data/shared/msalim/production-runs/bba_28_case2
    initial_h5_transfer_dir: /lus/theta-fs0/projects/RL-fold/braceal/1FME-training-seed/
    mode: train
    num_frames_per_training: 10240 # 160 files (64 per file)
    run_script: /data/shared/vishal/ANL-shared/cvae_gb/run_mixed.sh
    runconfig_params:
        keep_checkpoint_max: 1
        log_step_count_steps: 32
        save_checkpoints_steps: 320
        save_summary_steps: 32
    train_steps: 1600 # 50 epochs, 1 epoch is 16384 samples->32 steps

cvae_model:
    activation: relu
    metrics: False
    allowed_optimizers:
        - sgd
        - sgdm
        - adam
        - rmsprop
    batch_size: 512
    beta1: 0.2
    beta2: 0.9
    data_random_seed: null
    dec_conv_filters:
        - 100
        - 100
        - 100
        - 100
    dec_conv_kernels:
        - 5
        - 5
        - 5
        - 5
    dec_conv_strides:
        - 1
        - 1
        - 1
        - 1
    decay: 0.9
    dense_units: 64
    enc_conv_filters:
        - 100
        - 100
        - 100
        - 100
    enc_conv_kernels:
        - 5
        - 5
        - 5
        - 5
    enc_conv_strides:
        - 1
        - 1
        - 1
        - 1
    epsilon: 1.0e-08
    fraction: 0.2
    full_precision_loss: false
    h5_shape:
        - 1
        - 28
        - 28
    input_shape:
        - 1
        - 28
        - 28
    itemsize: 1
    kl_loss_reduction_type: sum
    last_n_files: 16 # 1024 samples per file, per epoch 16384 samples
    last_n_files_eval: 4 # 1024 samples per file, per epoch 4096 samples
    latent_ndim: 10
    learning_rate: 2.0e-05
    loss_scale: 1.0
    mixed_precision: true
    model_random_seed: null
    momentum: 0.9
    optimizer_name: rmsprop
    reconstruction_loss_reduction_type: sum
    samples_per_file: 1024 # 10 files per transfer (8 train, 2 eval)
    tfrecord_shape:
        - 1
        - 28
        - 28

gpu_training: null

logging:
    buffer_num_records: 1024
    datefmt: "%d-%b-%Y %H:%M:%S"
    flush_period: 30
    format:
        "%(asctime)s|%(process)d|%(thread)d|%(levelname)8s|%(name)s:%(lineno)s]
        %(message)s"
    level: DEBUG
