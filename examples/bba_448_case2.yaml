experiment_directory: /path/to/experiment
walltime_min: 120

md_runner:
    frames_per_h5: 128
    initial_configs_dir: /path/to/initial_pdbs_and_tops
    local_run_dir: /raid/scratch
    md_environ_setup:
        - eval "$(/lus/theta-fs0/projects/RL-fold/msalim/miniconda3/bin/conda shell.bash
          hook)"
        - conda activate /lus/theta-fs0/projects/RL-fold/venkatv/software/conda_env/a100_rapids_openmm
    md_run_command: python run_openmm.py
    num_jobs: 10
    reeval_time_ns: 2.0
    reference_pdb_file: /path/to/reference.pdb
    report_interval_ps: 50.0
    sim_type: explicit
    simulation_length_ns: 10.0

outlier_detection:
    environ_setup: []
    extrinsic_outlier_score: none
    gpus_per_node: 8
    local_scratch_dir: /raid/scratch
    max_num_old_h5_files: 1000
    num_nodes: 1
    num_outliers: 500
    ranks_per_node: 1
    run_command: python -m deepdrivemd.outlier.lof
    sklearn_num_cpus: 16

cs1_training: # start with 10240 and supplement with 10240 at a time (2048 eval, 8192 train)
    eval_steps: 8 # 512*8=4096 samples
    hostname: medulla1
    medulla_experiment_path: /data/shared/experiment/on/medulla1
    metrics: false
    mode: train
    num_frames_per_training: 10240 # 80 files
    run_script: /data/shared/vishal/ANL-shared/cvae_gb/run_mixed.sh
    runconfig_params:
        keep_checkpoint_max: 1
        log_step_count_steps: 32
        save_checkpoints_steps: 320
        save_summary_steps: 32
    train_steps: 640 # 20 epochs, 1 epoch is 16384 samples->32 steps

cvae_model:
    activation: relu
    allowed_optimizers:
        - sgd
        - sgdm
        - adam
        - rmsprop
    batch_size: 512
    beta1: 0.2
    beta2: 0.9
    data_random_seed: null
    dec_conv_filters:
        - 100
        - 100
        - 100
        - 100
    dec_conv_kernels:
        - 5
        - 5
        - 5
        - 5
    dec_conv_strides:
        - 2
        - 2
        - 2
        - 2
    decay: 0.9
    dense_units: 64
    enc_conv_filters:
        - 100
        - 100
        - 100
        - 100
    enc_conv_kernels:
        - 5
        - 5
        - 5
        - 5
    enc_conv_strides:
        - 2
        - 2
        - 2
        - 2
    epsilon: 1.0e-08
    fraction: 0.2
    full_precision_loss: false
    h5_shape:
        - 1
        - 448
        - 448
    input_shape:
        - 1
        - 448
        - 448
    itemsize: 1
    kl_loss_reduction_type: sum
    last_n_files: 16 # 1024 samples per file, per epoch 16384 samples
    last_n_files_eval: 4 # 1024 samples per file, per epoch 4096 samples
    latent_ndim: 10
    learning_rate: 2.0e-05
    loss_scale: 1.0
    mixed_precision: true
    model_random_seed: null
    momentum: 0.9
    optimizer_name: rmsprop
    reconstruction_loss_reduction_type: sum
    samples_per_file: 1024
    tfrecord_shape:
        - 1
        - 448
        - 448

gpu_training: null

logging:
    buffer_num_records: 1024
    datefmt: "%d-%b-%Y %H:%M:%S"
    flush_period: 30
    format:
        "%(asctime)s|%(process)d|%(thread)d|%(levelname)8s|%(name)s:%(lineno)s]
        %(message)s"
    level: DEBUG
